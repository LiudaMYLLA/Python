# Task 1: Import necessary libraries and load data
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.model_selection import cross_val_score, KFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn import model_selection
# Suppress warnings for clean output
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
data = pd.read_csv('bank-additional-full.csv', sep=';')  # Ensure correct separator

# Display basic statistical information about numerical variables
print("Basic Statistical Information:")
print(data.describe())

# Display dataset dimensions (number of rows and columns)
print("\nDataset Dimensions:")
print(data.shape)

# Task 2: Identifying and encoding the target variable

# Check unique values in the target variable 'y'
print("\nUnique values in target variable 'y':", data['y'].unique())

# Convert target variable 'y' from categorical to numerical (yes=1, no=0)
data['y'] = data['y'].map({'yes': 1, 'no': 0})

# Verify the conversion
print("\nAfter Encoding Target Variable 'y':")
print(data['y'].value_counts())



# Task 3: Extract attributes that characterize bank clients
bank_client = data[['age', 'job', 'marital', 'education', 'default', 'housing', 'loan']]

# Display unique values for each categorical attribute
for column in bank_client.columns:
    print(f"\nUnique values in {column}:")
    print(bank_client[column].unique())

    import matplotlib.pyplot as plt
    import seaborn as sns



# Task 4: Basic statistics of 'age'
    print("\nAge Analysis:")
    print(f"Maximum Age: {bank_client['age'].max()}")
    print(f"Minimum Age: {bank_client['age'].min()}")

    # Check for missing values
    print(f"\nMissing Values in 'Age': {bank_client['age'].isnull().sum()}")

    # Visualization of the 'age' variable
    plt.figure(figsize=(16, 4))

    # Countplot of Age
    plt.subplot(1, 3, 1)
    sns.countplot(x=bank_client['age'], palette="coolwarm")
    plt.title("Countplot of Age")
    plt.xticks(rotation=90)

    # Boxplot of Age (Checking for Outliers)
    plt.subplot(1, 3, 2)
    sns.boxplot(x=bank_client['age'], palette="coolwarm")
    plt.title("Boxplot of Age")

    # Distribution plot of Age
    plt.subplot(1, 3, 3)
    sns.histplot(bank_client['age'], bins=30, kde=True, color='blue')
    plt.title("Distribution of Age (Distplot)")

    plt.tight_layout()
    plt.show()




# Task 5: Detect Outliers Using Interquartile Range
def detect_outliers_with_interquantile_range(df: pd.DataFrame,
                                             column: str,
                                             low_percentile: int,
                                             high_percentile: int) -> pd.DataFrame:
    """
    Detecting outliers using interquartile range manually.

    df: pd.DataFrame - dataset to analyze
    column: str - target column
    low_percentile: int - lower bound percentile (0-100)
    high_percentile: int - upper bound percentile (0-100)

    returns: pd.DataFrame with detected outliers
    """

    sorted_data = np.sort(df[column])

    # Calculate Q1 and Q3
    Q1 = np.percentile(sorted_data, low_percentile)
    Q3 = np.percentile(sorted_data, high_percentile)

    print('')
    print(f"Q1 ({low_percentile} percentile) of the given data: {Q1}")
    print(f"Q3 ({high_percentile} percentile) of the given data: {Q3}")
    print('----------------------------------------------------------')

    # Calculate IQR
    IQR = Q3 - Q1
    print(f"Interquartile Range (IQR): {IQR}")
    print('----------------------------------------------------------')

    # Calculate lower and upper bounds
    low_lim = Q1 - 1.5 * IQR
    up_lim = Q3 + 1.5 * IQR

    print(f"Lower bound: {low_lim}")
    print(f"Upper bound: {up_lim}")
    print('----------------------------------------------------------')

    # Identify outliers
    outliers = df[(df[column] > up_lim) | (df[column] < low_lim)]

    print(f"Outliers detected in '{column}':")
    print(outliers)  # Replacing display(outliers) with print(outliers)

    return outliers

# Run the function for 'age'
outliers = detect_outliers_with_interquantile_range(bank_client, 'age', 10, 90)

# Task 6: Visualizing 'job', 'marital', and 'education' attributes

plt.figure(figsize=(18, 5))

# Job Distribution
plt.subplot(1, 3, 1)
sns.countplot(y=bank_client['job'], order=bank_client['job'].value_counts().index, palette="coolwarm")
plt.title("Distribution of Job Categories")
plt.xlabel("Count")
plt.ylabel("Job Type")

# Marital Status Distribution
plt.subplot(1, 3, 2)
sns.countplot(x=bank_client['marital'], palette="coolwarm")
plt.title("Distribution of Marital Status")
plt.xlabel("Marital Status")
plt.ylabel("Count")

# Education Distribution
plt.subplot(1, 3, 3)
sns.countplot(y=bank_client['education'], order=bank_client['education'].value_counts().index, palette="coolwarm")
plt.title("Distribution of Education Levels")
plt.xlabel("Count")
plt.ylabel("Education Level")

plt.tight_layout()
plt.show()


# Task 7: Visualizing 'default', 'housing', and 'loan' attributes
plt.figure(figsize=(18, 5))

# Default Status
plt.subplot(1, 3, 1)
sns.countplot(x=bank_client['default'], palette="coolwarm")
plt.title("Distribution of Default Status")
plt.xlabel("Default")
plt.ylabel("Count")

# Housing Loan Status
plt.subplot(1, 3, 2)
sns.countplot(x=bank_client['housing'], palette="coolwarm")
plt.title("Distribution of Housing Loan Status")
plt.xlabel("Housing Loan")
plt.ylabel("Count")

# Personal Loan Status
plt.subplot(1, 3, 3)
sns.countplot(x=bank_client['loan'], palette="coolwarm")
plt.title("Distribution of Personal Loan Status")
plt.xlabel("Loan Status")
plt.ylabel("Count")

plt.tight_layout()
plt.show()

from sklearn.preprocessing import LabelEncoder


# Task 8: Encoding categorical features
def encode_categorical_columns(df: pd.DataFrame, categorical_cols: list) -> pd.DataFrame:
    """
    Function for encoding categorical features using LabelEncoder

    df: pd.DataFrame to encode
    categorical_cols: specified columns to encode

    returns: pd.DataFrame with encoded values
    """
    for col in categorical_cols:
        le = LabelEncoder()
        not_null = df[col][df[col].notnull()]  # Ensure we only transform non-null values
        df[col] = le.fit_transform(not_null)  # Apply label encoding

    return df


# Encoding selected categorical columns
bank_client = encode_categorical_columns(bank_client, ['job', 'marital', 'education', 'default', 'housing', 'loan'])

# Display transformed data
print(bank_client.head())


# Task 9: Encoding Age based on Quartile Bins
def encode_age(dataframe: pd.DataFrame, age_col: str) -> pd.DataFrame:
    """
    Function for encoding age column using quartiles

    df: pd.DataFrame to encode
    age_col: age column to encode

    returns: pd.DataFrame with transformed age values
    """
    # Define quartiles
    dataframe['age_group'] = pd.qcut(dataframe[age_col], q=4, labels=[1, 2, 3, 4])

    # Drop the original 'age' column after transformation
    dataframe = dataframe.drop(columns=[age_col])

    return dataframe


# Apply age transformation
bank_client = encode_age(bank_client, 'age')

# Display the updated dataset
print(bank_client.head())


# Task 10: Extracting client-bank contact attributes
bank_relat = data[['contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays', 'previous', 'poutcome']]

# Display the extracted DataFrame
print("Client-Bank Contact DataFrame:")
print(bank_relat.head())

# Check for missing values in these variables
print("\nMissing Values in Client-Bank Contact Attributes:")
print(bank_relat.isnull().sum())


# Task 11: Display unique values for 'contact', 'month', and 'day_of_week'
for column in ['contact', 'month', 'day_of_week']:
    print(f"\nUnique values in '{column}': {bank_relat[column].unique()}")




# Task 12: Visual Analysis of 'duration'
plt.figure(figsize=(14, 6))

# Boxplot to check for outliers
plt.subplot(1, 2, 1)
sns.boxplot(x=bank_relat['duration'], color='skyblue')
plt.title("Boxplot of Call Duration")

# Distribution plot (Histogram + KDE)
plt.subplot(1, 2, 2)
sns.histplot(bank_relat['duration'], bins=50, kde=True, color='blue')
plt.title("Distribution of Call Duration")

plt.tight_layout()
plt.show()


# Task 13: Encode categorical variables 'contact', 'month', 'day_of_week'
bank_relat = encode_categorical_columns(bank_relat, ['contact', 'month', 'day_of_week'])

# Display first few rows after encoding
print(bank_relat.head())


# Task 14: Encoding 'duration' using quartiles
def encode_duration(dataframe: pd.DataFrame, duration_col: str) -> pd.DataFrame:
    """
    Function for encoding 'duration' column into quartiles.

    dataframe: pd.DataFrame to encode
    duration_col: column name for duration

    returns: transformed DataFrame
    """
    # Define quartiles
    dataframe['duration_group'] = pd.qcut(dataframe[duration_col], q=4, labels=[1, 2, 3, 4])

    # Drop the original 'duration' column
    dataframe = dataframe.drop(columns=[duration_col])

    return dataframe


# Apply duration transformation
bank_relat = encode_duration(bank_relat, 'duration')

# Display transformed dataset
print(bank_relat.head())


# Task 15: Extract Social and Economic Context Attributes
bank_socec = data[['emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']]

# Display first few rows
print("Social and Economic Context DataFrame:")
print(bank_socec.head())

# Check for missing values
print("\nMissing Values in Social and Economic Attributes:")
print(bank_socec.isnull().sum())

# Task 16: Extract 'campaign', 'pdays', 'previous', and 'poutcome'
bank_other = data[['campaign', 'pdays', 'previous', 'poutcome']]

# Display first few rows
print("Other Attributes DataFrame:")
print(bank_other.head())

# Check for missing values
print("\nMissing Values in Other Attributes:")
print(bank_other.isnull().sum())


# Task 17: Transform 'poutcome' into numerical values
bank_other['poutcome'] = bank_other['poutcome'].map({'nonexistent': 0, 'failure': 1, 'success': 2})

# Verify encoding
print("\nTransformed 'poutcome' Attribute:")
print(bank_other['poutcome'].value_counts())
from sklearn.preprocessing import LabelEncoder


# Task 18: Merge all processed datasets into one final DataFrame

# Task 18: Merge all processed datasets into one final DataFrame
bank_final = pd.concat([bank_client, bank_relat, bank_socec, bank_other], axis=1)

# Ensure all columns are numeric before scaling
print("Checking for non-numeric columns in bank_final:")
non_numeric_cols = bank_final.select_dtypes(include=['object']).columns
print("Non-numeric columns:", list(non_numeric_cols))

# Check if 'poutcome' is numeric and exclude if already transformed
if 'poutcome' in non_numeric_cols:
    print("\nChecking 'poutcome' column before encoding:")
    print(bank_final['poutcome'].head())
    print("Data type of 'poutcome':", bank_final.dtypes['poutcome'])
    # If it's numeric, remove it from the list of non-numeric columns
    if pd.api.types.is_numeric_dtype(bank_final['poutcome']):
        non_numeric_cols = [col for col in non_numeric_cols if col != 'poutcome']

# Corrected encode_categorical_columns function:
def encode_categorical_columns(df, categorical_cols):
    """
    Encodes categorical columns using LabelEncoder.

    df: DataFrame with categorical variables
    categorical_cols: List of column names to encode

    returns: DataFrame with encoded values
    """
    le = LabelEncoder()

    # Apply LabelEncoder to each column in the list
    for col in categorical_cols:
        # Ensure it's a valid 1D array/Series before encoding
        if df[col].ndim == 1:
            df[col] = le.fit_transform(df[col].astype(str))
        else:
            print(f"Skipping column {col} because it is not 1D.")
    return df

# Apply encoding to any remaining categorical columns
if len(non_numeric_cols) > 0:
    # Use the corrected function to process each column
    bank_final = encode_categorical_columns(bank_final, non_numeric_cols)

# Verify final structure
print("Final DataFrame Structure:")
print(bank_final.info())

# Task 19: Prepare the data for training
def prepare_data_for_training(X: pd.DataFrame, target: pd.Series, test_size: float, n_splits: int, random_state: int):
    """
    Splits the data into train/test sets and applies K-Fold cross-validation.

    X: Feature DataFrame (must be numeric)
    target: Target variable (binary classification)
    test_size: Proportion of test dataset
    n_splits: Number of K-Fold splits
    random_state: Random seed

    returns: X_train, X_test, y_train, y_test, k_fold
    """
    # Ensure X is fully numeric
    X = X.apply(pd.to_numeric, errors='coerce')

    # Split the dataset
    X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=test_size, random_state=random_state, stratify=target)

    # Define K-Fold cross-validation
    k_fold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)

    return X_train, X_test, y_train, y_test, k_fold

# Apply function to split data
X_train, X_test, y_train, y_test, k_fold = prepare_data_for_training(bank_final, data['y'], 0.2, 10, 101)

# Verify data dimensions
print(f"Training set size: {X_train.shape}, Testing set size: {X_test.shape}")


# Task 20: Scale the Data
def scale_the_data(X_train: pd.DataFrame, X_test: pd.DataFrame):
    """
    Standardizes features by removing the mean and scaling to unit variance.

    X_train: Training feature set
    X_test: Testing feature set

    returns: Scaled X_train and X_test
    """
    sc_X = StandardScaler()

    # Convert DataFrame to Numeric (ensure no categorical values remain)
    X_train = X_train.apply(pd.to_numeric, errors='coerce')
    X_test = X_test.apply(pd.to_numeric, errors='coerce')

    # Scale the data
    X_train = sc_X.fit_transform(X_train)
    X_test = sc_X.transform(X_test)

    return X_train, X_test

# Apply scaling
X_train, X_test = scale_the_data(X_train, X_test)


from sklearn.impute import SimpleImputer

from sklearn import model_selection  # This import is necessary

# Task 21: Train Logistic Regression Model

# Impute missing values (e.g., with mean)
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Now fit the Logistic Regression model
logmodel = LogisticRegression()
logmodel.fit(X_train_imputed, y_train)

# Make predictions
logpred = logmodel.predict(X_test_imputed)

# Model evaluation
print("Confusion Matrix (Logistic Regression):")
print(confusion_matrix(y_test, logpred))
print(f"Logistic Regression Accuracy: {round(accuracy_score(y_test, logpred) * 100, 2)}%")

# Cross-validation score
LOGCV = cross_val_score(logmodel, X_test_imputed, y_test, cv=k_fold, n_jobs=1, scoring='accuracy').mean()
print(f"Logistic Regression Cross-Validation Score: {LOGCV:.2f}")

# Task 22: Determine the Optimal Number of Neighbors for k-NN

# Impute missing values for k-NN
X_train_imputed_knn = imputer.fit_transform(X_train)
X_test_imputed_knn = imputer.transform(X_test)

neighbors = np.arange(1, 26)  # Testing values from 1 to 25
cv_scores = []  # Store cross-validation scores

# Perform 10-fold cross-validation for each value of k
for k in neighbors:
    knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', metric='euclidean')
    scores = model_selection.cross_val_score(knn, X_train_imputed_knn, y_train, cv=k_fold, scoring='accuracy')
    cv_scores.append(scores.mean() * 100)
    print(f"k={k} Accuracy: {scores.mean() * 100:.2f}% (+/- {scores.std() * 100:.2f}%)")

# Identify the best k-value
optimal_k = neighbors[np.argmax(cv_scores)]
print(f"\nOptimal Number of Neighbors: {optimal_k} with {max(cv_scores):.1f}% accuracy")

# Plot the accuracy trend
plt.plot(neighbors, cv_scores, marker='o')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Cross-Validation Accuracy (%)')
plt.title('k-NN Hyperparameter Tuning')
plt.show()

# Train k-NN using the optimal k
knn = KNeighborsClassifier(n_neighbors=optimal_k)
knn.fit(X_train_imputed_knn, y_train)

# Make predictions
knnpred = knn.predict(X_test_imputed_knn)

# Evaluate k-NN model
print("Confusion Matrix (k-NN):")
print(confusion_matrix(y_test, knnpred))
print(f"k-NN Accuracy: {round(accuracy_score(y_test, knnpred) * 100, 2)}%")

# Cross-validation score
KNNCV = cross_val_score(knn, X_test_imputed_knn, y_test, cv=k_fold, n_jobs=1, scoring='accuracy').mean()
print(f"k-NN Cross-Validation Score: {KNNCV:.2f}")

# Task 23: Compare Logistic Regression and k-NN Models
models = pd.DataFrame({
    'Models': ['Logistic Regression', 'k-Nearest Neighbors'],
    'Score': [LOGCV, KNNCV]
})

# Sort by accuracy
models = models.sort_values(by='Score', ascending=False)
print("\nModel Performance Comparison:")
print(models)
